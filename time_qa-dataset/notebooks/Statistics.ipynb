{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datasets import Dataset, VerificationMode, concatenate_datasets, load_dataset\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option #1: Load data from huggingface "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `huggingface-cli login` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeQADataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        task: Literal[\"binary\", \"multi\", \"open\"],\n",
    "        batch_size: int = 32,\n",
    "        repo: str = \"dasyd/time-qa\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.task = task\n",
    "        self.batch_size = batch_size\n",
    "        self.repo = repo\n",
    "\n",
    "    def _load_dataset_split(self, splits: list[str]):\n",
    "        \"\"\"Workaround to overcome the missing hf implementation of only dowloading the split shards\"\"\"\n",
    "\n",
    "        return load_dataset(\n",
    "            self.repo,\n",
    "            self.task,\n",
    "            data_dir=self.task,\n",
    "            data_files={split: f\"{split}-*\" for split in splits},\n",
    "            verification_mode=VerificationMode.NO_CHECKS,\n",
    "            num_proc=len(splits),\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download all, since only this is run on the main process\n",
    "        self._load_dataset_split([\"train\", \"val\", \"test\"])\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            self.dataset = self._load_dataset_split([\"train\", \"val\"])\n",
    "        elif stage == \"test\":\n",
    "            self.dataset = self._load_dataset_split([\"test\"])\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset[\"train\"], batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset[\"val\"], batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset[\"test\"], batch_size=self.batch_size)\n",
    "\n",
    "    def all_splits(self) -> Dataset:\n",
    "        return concatenate_datasets(self._load_dataset_split([\"train\", \"val\", \"test\"]).values())\n",
    "\n",
    "\n",
    "module = TimeQADataModule(task=\"binary\")  # TODO: load all variants (binary, multi, open)!\n",
    "module.prepare_data()\n",
    "complete = module.all_splits()\n",
    "complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(complete)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\n",
    "            \"sample_id\": entry[\"sample_id\"],\n",
    "            \"question_id\": entry[\"question_id\"],\n",
    "            \"textual_description\": entry[\"textual_description\"],\n",
    "            \"question_type\": entry[\"question_type\"],\n",
    "            \"question\": entry[\"question\"],\n",
    "            \"answer_type\": entry[\"answer_type\"],\n",
    "            \"answer\": entry[\"answer\"],\n",
    "            \"action_sequence\": [],  # Not available in the dataset\n",
    "        }\n",
    "        for entry in tqdm(complete)\n",
    "    ],\n",
    ")\n",
    "pd.to_pickle(data, \"textual_part.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"textual_part.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option #2: Load local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\".\").absolute().parents[1] / \"generated-dataset-30_000\" / \"data\"\n",
    "\n",
    "assert base_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instances():\n",
    "    for file in base_path.glob(\"*/data.json\"):\n",
    "        with open(file) as f:\n",
    "            as_json = json.load(f)\n",
    "\n",
    "        def answer_mapping(answer_type, answer, options, correct_option):\n",
    "            match answer_type:\n",
    "                case \"binary\":\n",
    "                    correct = str(options[correct_option])\n",
    "                    return {\"True\": \"A\", \"False\": \"B\"}[correct]\n",
    "                case \"multi\":\n",
    "                    return correct_option\n",
    "                case _:\n",
    "                    return answer\n",
    "\n",
    "        action_sequence = [entry[\"action\"] for entry in as_json[\"prompt_sequence\"]]\n",
    "\n",
    "        yield from (\n",
    "            {\n",
    "                \"sample_id\": str(file.parts[-2]),\n",
    "                \"question_id\": question_id,\n",
    "                \"textual_description\": as_json[\"textual_description\"],\n",
    "                \"question_type\": qa_pair[\"question_type\"],\n",
    "                \"question\": qa_pair[\"question\"],\n",
    "                \"answer_type\": qa_pair[\"answer_type\"],\n",
    "                \"answer\": answer_mapping(\n",
    "                    qa_pair[\"answer_type\"], qa_pair[\"answer\"], qa_pair[\"options\"], qa_pair[\"correct_option\"]\n",
    "                ),\n",
    "                \"action_sequence\": action_sequence,\n",
    "            }\n",
    "            for question_id, qa_pair in enumerate(as_json[\"qa_pairs\"])\n",
    "        )\n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_records(list(tqdm(get_instances())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(\n",
    "    columns={\n",
    "        \"textual_description\": \"Description\",\n",
    "        \"question_type\": \"Question Type\",\n",
    "        \"question\": \"Question\",\n",
    "        \"answer_type\": \"Answer Type\",\n",
    "        \"answer\": \"Answer\",\n",
    "        \"action_sequence\": \"Action Sequence\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=[\"Answer Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"unique_id\"] = data[\"sample_id\"].astype(str) + \"_\" + data[\"question_id\"].astype(str)\n",
    "data[\"Action Sequence\"] = data[\"Action Sequence\"].apply(lambda x: \" - \".join(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = data[data[\"Answer Type\"] == \"binary\"]\n",
    "multi = data[data[\"Answer Type\"] == \"multi\"]\n",
    "binary_multi = pd.concat([binary, multi])\n",
    "open_ = data[data[\"Answer Type\"] == \"open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "g = sns.displot(\n",
    "    data=binary_multi.sort_values(by=[\"Answer Type\", \"Question Type\", \"Answer\"]),\n",
    "    x=\"Answer\",\n",
    "    hue=\"Answer Type\",\n",
    "    col=\"Question Type\",\n",
    "    col_wrap=5,\n",
    ")\n",
    "g.set_titles(\"{col_name}\")\n",
    "sns.move_legend(g, \"lower center\", bbox_to_anchor=(0.5, 1), ncol=3, frameon=True)\n",
    "plt.savefig(\"answer_distribution.pdf\", bbox_inches=\"tight\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 14))\n",
    "ax = sns.histplot(\n",
    "    data=data.sort_values(by=[\"Answer Type\", \"Question Type\"]),\n",
    "    y=\"Question Type\",\n",
    "    hue=\"Answer Type\",\n",
    "    # shrink=0.75,\n",
    ")\n",
    "sns.move_legend(ax, \"lower center\", bbox_to_anchor=(0.5, 1), ncol=3, frameon=True)\n",
    "plt.savefig(\"question_distribution.pdf\", bbox_inches=\"tight\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique = data.copy()\n",
    "data_unique[\"Q&A\"] = data_unique[\"Question\"] + \" \" + data_unique[\"Answer\"]\n",
    "data_unique.nunique()[[\"Description\", \"Q&A\", \"Question Type\", \"Question\", \"Answer\", \"Action Sequence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from generate.prompts.base.utility import all_actions\n",
    "# len(all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
