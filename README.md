#  QuAnTS: Question Answering on Time Series – Dataset Generation

![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)

[![Dataset on HF](https://huggingface.co/datasets/huggingface/badges/resolve/main/dataset-on-hf-sm.svg)](https://huggingface.co/datasets/dasyd/quants)

This repository holds the code for generating the multimodal QuAnTS dataset, comprised of time series of human activities and textual questions and answers.
The purpose of the generated dataset is to lay the groundwork for a system that can answer questions based on time series data by examining how to produce a synthetic text and time series dataset that corresponds to real-world activities.
Question and answer pairs are generated automatically from predefined question categories.

Further information regarding the question categories can be found in the corresponding paper: *To be published*

## Usage of this repository

### Installation

Use git to clone this repository:
```shell
git clone https://github.com/mauricekraus/quants-generate.git
```

The codebase is written in Python.
Run the following commands to install the dependencies and the library itself (assuming you have Python 3.10+ and pip installed).
You also need `ffmpeg` for video conversion.
The dataset generation commands assume you are in the `quants/` folder.

```shell
apt install ffmpeg
pip install -e quants[dev]
```

This repository also provides a Docker image and a [devcontainer](https://containers.dev/) for easy experimentation.

### Overview

Run `python -m generate --help` to get an overview of the available commands:

```txt
╭─ Commands ──────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ generate-motion-stmc    Run STMC to obtain the motion trajectories via diffusion.                                   │
│ prompts                 Generate a custom time series question-answering dataset (textual components only).         │
│ render-priormdm         Animate the human motion trajectories generated by PriorMDM (legacy).                       │
│ render-stmc             Animate the human motion trajectories generated by STMC.                                    │
│ transcode-animations    Transcode the MP4 animations to a much more compressed format.                              │
╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
```

### Running the prompt generation

To execute the code, run `python -m generate prompts --num-samples 1000`. This will generate a new folder (by default `generated_dataset/`) containing JSON files containing a prompt sequence and the matching question and answer pairs. The program will print the first few instances as tables to the console.

To adjust the parameters of the generation process, run the same code with `--help`.

#### Changing the generation code

The file `quants/generate/prompts/base/list_of_actions_evaluated.txt` contains all possible actions.

The code is in the folder `quants/generate/prompts/`, which contains three sub-folders. The folder `answers/` holds the generated answer templates, and the `qna_type/` folder has the question templates and the code for each question category in the corresponding subfolders. The `base/` contains data types and utility helpers with help functions and lists of possible actions used to generate the dataset.

The text files are the templates for the generated questions, answers, actions, and descriptions for the prompts. Values like `{activity}` and `{time}` are used as placeholders and will be filled in the code.
Note that the available placeholders are specific to each template.
Empty lines, whitespace-only lines, and those starting with a `#` will be ignored.
Furthermore, duplicates are filtered out.

#### Obtaining a simplified dataset variant

Set the `question_type_distribution` to `[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]` (e.g., for only generating `AfterQuestion` and `ActionCountQuestion`).
Then run `python -m generate prompts --answer-types binary --num-samples 500`

### Running the motion diffusion

Please use `python -m generate render-stmc --help` to see the available options.

### Visualizing the dataset

To instance of the dataset, run `python -m generate render-stmc --help` to see the available options.
By default, it loads the data from HuggingFace. Log in with `huggingface-cli login` or provide a token as an environment variable or argument.
Alternatively, you can generate from a local folder by providing `--input-path my-own-quants-dataset/data`.
To run it for multiple at once, run something like `for i in {0..10}; do python -m generate render --idx $i --no-ask-for-token; done`.

### Development tricks

You should commit/backup your files before running the following commands.

- Count the number of lines in all question template `.txt`-files: `find generate/prompts/qna_type/ -type f -name "*.txt" -exec wc -l {} \; | sort -k1,1n`
- Remove duplicates from all template `.txt`-files: `find generate/prompts -type f -name "*.txt" -exec sh -c 'awk "!seen[\$0]++" {} > temp && mv temp {}' \;`
- Remove trailing whitespace from all template `.txt`-files: `find generate/prompts -type f -name "*.txt" -exec sed -i 's/[ \t]*$//' {} \;`
- To convert all videos from MP4 to GIF and add timestamps, run `for file in videos/*; do export VID_FULL=${file##*/}; export VID=${VID_FULL%.mp4}; ffmpeg -i videos/$VID.mp4 -vf "fps=10,scale=320:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse,drawtext=fontfile=/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf:expansion=normal: text='%{pts \\: hms}': fontcolor=white:fontsize=18: x=(w-text_w)/2: y=h-th-10: box=1: boxcolor=black: boxborderw=5: line_spacing=32" -loop 0 videos-gifs/$VID.gif ; done`.

## Possible future improvements

- LLMs could be used to generate prompts with more variety or for rephrasing existing ones.
- The consistency of the generated prompts could be improved by using a common tense (present or past). However, as long as each prompt is consistent, the generated dataset should be fine and may benefit from the diversity. That is the current approach.
  However, questions and answers are generated separately, so there is currently no guarantee that the tense is consistent between them.
- Allow for multiple answers per question. This would be useful for questions like `"What is an activity following some time after the activity X?"` (`after_open`).
- Many generators correctly handle synonyms, but some do not because this is not present in the current set of possible actions. Before adding synonyms, checking if each generator handles them correctly would be necessary.

QuAnTS is inspired by the style of [ScienceQA](https://huggingface.co/datasets/derek-thomas/ScienceQA?row=1).

## License and Citation

This repository is licensed under the MIT License.
See the [LICENSE](LICENSE) file for details.
This repository contains a copy of [the STMC codebase](https://github.com/nv-tlabs/stmc).

For using the dataset and citing it, please see [the huggingface dataset repository](https://huggingface.co/datasets/dasyd/quants#licensing-citation-and-acknowledgments).
